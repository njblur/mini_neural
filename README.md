A tiny neural network implementation

the codes here are mainly for the purpose to fullfil the concept and mathatical theory of neural networks.
they are all writen from scratch with only numpy as math lib.
the normal neural network has been implemented with following activation and loss functions: liner,sigmoid,softmax.
weights decay(L2 regulazation) is also touched. 
an mnist example is made based on this experimental neural nets. 
A simple LSTM is implemented also but with no sequence,no batch yet.
test codes take the example by char rnn from karpathy.
some codes are taken from other peoples for compare
also an char rnn example are made based on tensorflow to compare with mine.

contact me at jianwu.hu@gmail.com if you have any interesting
